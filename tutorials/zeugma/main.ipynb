{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: KBC Initialization\n",
    "\n",
    "In this first phase of `Fonduer`'s pipeline, `Fonduer` uses a user specified _schema_ to initialize a relational database where the output KB will be stored. Furthermore, `Fonduer` iterates over its input _corpus_ and transforms each document into a unified data model, which captures the variability and multimodality of richly formatted data. This unified data model then servers as an intermediate representation used in the rest of the phases.\n",
    "\n",
    "This preprocessed data is saved to a database. Connection strings can be specified by setting the `SNORKELDB` environment variable. If no database is specified, then SQLite at `./snorkel.db` is created by default. However, to enabled parallel execution, we use PostgreSQL throughout this tutorial.\n",
    "\n",
    "We initialize several variables for convenience that define what the database should be called and what level of parallelization the `Fonduer` pipeline will be run with. In the code below, we use PostgreSQL as our database backend. \n",
    "\n",
    "Before you continue, please make sure that you have PostgreSQL installed and have created a new database named `zeugma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PARALLEL = 4 # assuming a quad-core machine\n",
    "ATTRIBUTE = \"zeugma\"\n",
    "\n",
    "os.environ['FONDUERDBNAME'] = ATTRIBUTE\n",
    "os.environ['SNORKELDB'] = 'postgres://weijiechen1994@localhost:5432/' + os.environ['FONDUERDBNAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Defining a Candidate Schema\n",
    "\n",
    "We first initialize a `SnorkelSession`, which manages the connection to the database automatically, and enables us to save intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from fonduer import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the _schema_ of the relation we want to extract. This must be a subclass of Candidate, and we define it using a helper function. Here, we define a binary relation which connects two Span objects of text. This is what creates the relation's database table if it does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer import candidate_subclass\n",
    "\n",
    "Cata_Labref = candidate_subclass('Cata_Labref', ['cata','labref'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "Next, we load the corpus of datasheets and transform them into the unified data model. Each datasheet has a PDF and HTML representation. Both representations are used in conjunction to create a robust unified data model with textual, structural, tabular, and visual modality information. Note that since each document is independent of each other, we can parse the documents in parallel. Note that parallel execution will not work with SQLite, the default database engine. We depend on PostgreSQL for this functionality.\n",
    "\n",
    "### Configuring an `HTMLPreprocessor`\n",
    "We start by setting the paths to where our documents are stored, and defining a `HTMLPreprocessor` to read in the documents found in the specified paths. `max_docs` specified the number of documents to parse. For the sake of this tutorial, we only look at 100 documents.\n",
    "\n",
    "**Note that you need to have run `download_data.sh` before executing these next steps or you won't have the documents needed for the tutorial.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer import HTMLPreprocessor, OmniParser\n",
    "\n",
    "docs_path = 'data/html/'\n",
    "pdf_path = 'data/pdf/'\n",
    "\n",
    "max_docs = float('inf')\n",
    "doc_preprocessor = HTMLPreprocessor(docs_path, max_docs=max_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring an `OmniParser`\n",
    "Next, we configure an `OmniParser`, which serves as our `CorpusParser` for PDF documents. We use [CoreNLP](https://stanfordnlp.github.io/CoreNLP/) as a preprocessing tool to split our documents into phrases and tokens, and to provide annotations such as part-of-speech tags and dependency parse structures for these phrases. In addition, we can specify which modality information to include in the unified data model for each document. Below, we enable all modality information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.5 s, sys: 36.6 ms, total: 1.54 s\n",
      "Wall time: 5min 29s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = OmniParser(structural=True, lingual=True, visual=True, pdf_path=pdf_path)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use simple database queries (written in the syntax of [SQLAlchemy](http://www.sqlalchemy.org/), which `Fonduer` uses) to check how many documents and phrases (sentences) were parsed, or even check how many phrases and tables are contained in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 11\n",
      "Phrases: 25358\n"
     ]
    }
   ],
   "source": [
    "from fonduer import Document, Phrase\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Phrases:\", session.query(Phrase).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dividing the Corpus into Test and Train\n",
    "\n",
    "We'll split the documents 80/10/10 into train/dev/test splits. Note that here we do this in a non-random order to preverse the consistency in the tutorial, and we reference the splits by 0/1/2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v2ch05',\n",
      " 'v2ch01',\n",
      " 'v2ch06',\n",
      " 'v2ch02',\n",
      " 'v2ch07',\n",
      " 'v2ch03',\n",
      " 'v2ch08',\n",
      " 'v2ch04',\n",
      " 'v2hawari-plates']\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.8, 0.9)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "pprint([x.name for x in train_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Candidate Extraction & Multimodal Featurization\n",
    "Given the unified data model from Phase 1, `Fonduer` extracts relation candidates based on user-provided **matchers** and **throttlers**. Then, `Fonduer` leverages the multimodality information captured in the unified data model to provide multimodal features for each candidate.\n",
    "\n",
    "## 2.1 Candidate Extraction\n",
    "\n",
    "The next step is to extract **candidates** from our corpus. A `candidate` is the object for which we want to make predictions. In this case, the candidates are pairs of transistor part numbers and their corresponding maximum storage temperatures as found in their datasheets. Our task is to predict which pairs are true in the associated document.\n",
    "\n",
    "To do so, we write **matchers** to define which spans of text in the corpus are instances of each entity. Matchers can leverage a variety of information from regular expressions, to dictionaries, to user-defined functions. Furthermore, different techniques can be combined to form higher quality matchers. In general, matchers should seek to be as precise as possible while maintaining complete recall.\n",
    "\n",
    "In our case, we need to write a matcher that defines a transistor part number and a matcher to define a valid temperature value.\n",
    "\n",
    "### Writing a simple temperature matcher\n",
    "\n",
    "Our maximum storage temperature matcher can be a very simple regular expression since we know that we are looking for integers, and by inspecting a portion of our corpus, we see that maximum storage temperatures fall within a fairly narrow range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer import RegexMatchSpan, DictionaryMatch, LambdaFunctionMatcher, Intersect, Union\n",
    "\n",
    "labref1_rgx = r'[0-9]{4,5}'\n",
    "labref2_rgx = r'[0-9]{4,5}.[0-9]{1,3}'\n",
    "lab_rgx = '|'.join([labref1_rgx, labref2_rgx])\n",
    "labref_matcher = RegexMatchSpan(rgx=lab_rgx, longest_match_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing an advanced transistor part matcher\n",
    "\n",
    "In contrast, transistor part numbers are complex expressions. Here, we show how transistor part numbers can leverage [naming conventions](https://en.wikipedia.org/wiki/Transistor#Part_numbering_standards.2Fspecifications) as regular expressions, and use a dictionary of known part numbers, and use user-defined functions together. First, we create a regular expression matcher for standard transistor naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer import RegexMatchSpan, DictionaryMatch, LambdaFunctionMatcher, Intersect, Union\n",
    "\n",
    "### Catalogue name and lab ref as Regular Expressions ###\n",
    "cata_rgx = r'(A|AM|B|BR|C|G|GD|IN|IR|L|LW|M|ML|PT|Q|SM|SS|ST|SV|SW|TC|TX|ZB)[0-9]{1,3}'\n",
    "cata_rgx_matcher = RegexMatchSpan(rgx=cata_rgx, longest_match_only=True)\n",
    "# part_matcher = Union(part_rgx_matcher, part_dict_matcher, part_file_name_matcher)\n",
    "cata_matcher = cata_rgx_matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Transistor Naming Conventions as Regular Expressions ###\n",
    "# eeca_rgx = r'([ABC][A-Z][WXYZ]?[0-9]{3,5}(?:[A-Z]){0,5}[0-9]?[A-Z]?(?:-[A-Z0-9]{1,7})?(?:[-][A-Z0-9]{1,2})?(?:\\/DG)?)'\n",
    "# jedec_rgx = r'(2N\\d{3,4}[A-Z]{0,5}[0-9]?[A-Z]?)'\n",
    "# jis_rgx = r'(2S[ABCDEFGHJKMQRSTVZ]{1}[\\d]{2,4})'\n",
    "# others_rgx = r'((?:NSVBC|SMBT|MJ|MJE|MPS|MRF|RCA|TIP|ZTX|ZT|ZXT|TIS|TIPL|DTC|MMBT|SMMBT|PZT|FZT|STD|BUV|PBSS|KSC|CXT|FCX|CMPT){1}[\\d]{2,4}[A-Z]{0,5}(?:-[A-Z0-9]{0,6})?(?:[-][A-Z0-9]{0,1})?)'\n",
    "\n",
    "# part_rgx = '|'.join([eeca_rgx, jedec_rgx, jis_rgx, others_rgx])\n",
    "# part_rgx_matcher = RegexMatchSpan(rgx=part_rgx, longest_match_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a relation's `ContextSpaces`\n",
    "\n",
    "Next, in order to define the \"space\" of all candidates that are even considered from the document, we need to define a `ContextSpace` for each component of the relation we wish to extract.\n",
    "\n",
    "In the case of transistor part numbers, the `ContextSpace` can be quite complex due to the need to handle implicit part numbers that are implied in text like \"BC546A/B/C...BC548A/B/C\", which refers to 9 unique part numbers. In addition, to handle these, we consider all n-grams up to 3 words long.\n",
    "\n",
    "In contrast, the `ContextSpace` for temperature values is simpler: we only need to process different unicode representations of a (`-`), and don't need to look at more than two works at a time.\n",
    "\n",
    "When no special preproessing like this is needed, we could have used the default `OmniNgrams` class provided by `snorkel.candidates`. For example, if we were looking to match polarities, which only take the form of \"NPN\" or \"PNP\", we could've used `attr_ngrams = OmniNgrams(n_max=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeugma_space import OmniNgramsCata, OmniNgramsLabref\n",
    "    \n",
    "cata_ngrams = OmniNgramsCata(parts_by_doc=None, n_max=1)\n",
    "labref_ngrams = OmniNgramsLabref(n_max=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining candidate `Throttlers`\n",
    "\n",
    "Next, we need to define **throttlers**, which allow us to further prune excess candidates and avoid unnecessarily materializing invalid candidates. Trottlers, like matchers, act as hard filters, and should be created to have high precision while maintaining complete recall, if possible.\n",
    "\n",
    "Here, we create a throttler that discards candidates if they are in the same table, but the part and storage temperature are not vertically or horizontally aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.lf_helpers import *\n",
    "import re\n",
    "\n",
    "def stg_temp_filter(c):\n",
    "    (part, attr) = c\n",
    "    if same_table((part, attr)):\n",
    "        return (is_horz_aligned((part, attr)) or is_vert_aligned((part, attr)))\n",
    "    return True\n",
    "\n",
    "candidate_filter = stg_temp_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the `CandidateExtractor`\n",
    "\n",
    "Now, we have all the component necessary to perform candidate extraction. We have defined the \"space\" of things to consider for each candidate, provided matchers that signal when a valid mention is seen, and a throttler to prunes away excess candidates. We now can define the `CandidateExtractor` with the contexts to extract from, the matchers, and the throttler to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process CandidateExtractorUDF-29:\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2380, in _flush\n",
      "    transaction.rollback(_capture_exception=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/snorkel/udf.py\", line 170, in run\n",
      "    self.session.commit()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 943, in commit\n",
      "    self.transaction.commit()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 467, in commit\n",
      "    self._prepare_impl()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 447, in _prepare_impl\n",
      "    self.session.flush()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2254, in flush\n",
      "    self._flush(objects)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/langhelpers.py\", line 66, in __exit__\n",
      "    compat.reraise(exc_type, exc_value, exc_tb)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/compat.py\", line 187, in reraise\n",
      "    raise value\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2344, in _flush\n",
      "    flush_context.execute()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/unitofwork.py\", line 391, in execute\n",
      "    rec.execute(self)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/unitofwork.py\", line 556, in execute\n",
      "    uow\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/persistence.py\", line 181, in save_obj\n",
      "    mapper, table, insert)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/persistence.py\", line 866, in _emit_insert_statements\n",
      "    execute(statement, params)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 948, in execute\n",
      "    return meth(self, multiparams, params)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py\", line 269, in _execute_on_connection\n",
      "    return connection._execute_clauseelement(self, multiparams, params)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1060, in _execute_clauseelement\n",
      "    compiled_sql, distilled_params\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1200, in _execute_context\n",
      "    context)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1416, in _handle_dbapi_exception\n",
      "    util.reraise(*exc_info)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/compat.py\", line 187, in reraise\n",
      "    raise value\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1193, in _execute_context\n",
      "    context)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/default.py\", line 507, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/usr/lib/python3.5/encodings/utf_8.py\", line 15, in decode\n",
      "    def decode(input, errors='strict'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Projects/Python/fonduer/fonduer/candidates.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, xs, split, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;34m\"\"\"Call the CandidateExtractorUDF.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCandidateExtractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Python/fonduer/fonduer/snorkel/udf.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, xs, clear, parallelism, progress_bar, count, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_st\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Python/fonduer/fonduer/snorkel/udf.py\u001b[0m in \u001b[0;36mapply_mt\u001b[0;34m(self, xs, parallelism, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Terminate and flush the processes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process CandidateExtractorUDF-32:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/snorkel/udf.py\", line 160, in run\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/candidates.py\", line 93, in apply\n",
      "    for tc in self.matchers[i].apply(self.candidate_spaces[i].apply(self.session, context)):\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/snorkel/matchers.py\", line 75, in apply\n",
      "    for c in candidates:\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/tutorials/zeugma/zeugma_space.py\", line 185, in apply\n",
      "    for ts in OmniNgrams.apply(self, session, context):\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/candidates.py\", line 160, in apply\n",
      "    doc = session.query(Document).filter(Document.id == context.id).one()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/query.py\", line 2848, in one\n",
      "    ret = self.one_or_none()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/query.py\", line 2818, in one_or_none\n",
      "    ret = list(self)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/query.py\", line 2888, in __iter__\n",
      "    self.session._autoflush()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 1434, in _autoflush\n",
      "    self.flush()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2254, in flush\n",
      "    self._flush(objects)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2380, in _flush\n",
      "    transaction.rollback(_capture_exception=True)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/langhelpers.py\", line 66, in __exit__\n",
      "    compat.reraise(exc_type, exc_value, exc_tb)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/compat.py\", line 187, in reraise\n",
      "    raise value\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2344, in _flush\n",
      "    flush_context.execute()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/unitofwork.py\", line 391, in execute\n",
      "    rec.execute(self)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/unitofwork.py\", line 556, in execute\n",
      "    uow\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/persistence.py\", line 181, in save_obj\n",
      "    mapper, table, insert)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/persistence.py\", line 830, in _emit_insert_statements\n",
      "    execute(statement, multiparams)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 948, in execute\n",
      "    return meth(self, multiparams, params)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py\", line 269, in _execute_on_connection\n",
      "    return connection._execute_clauseelement(self, multiparams, params)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1060, in _execute_clauseelement\n",
      "    compiled_sql, distilled_params\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1200, in _execute_context\n",
      "    context)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1416, in _handle_dbapi_exception\n",
      "    util.reraise(*exc_info)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/compat.py\", line 187, in reraise\n",
      "    raise value\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1170, in _execute_context\n",
      "    context)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/dialects/postgresql/psycopg2.py\", line 683, in do_executemany\n",
      "    cursor.executemany(statement, parameters)\n",
      "  File \"/usr/lib/python3.5/encodings/utf_8.py\", line 15, in decode\n",
      "    def decode(input, errors='strict'):\n",
      "KeyboardInterrupt\n",
      "Process CandidateExtractorUDF-31:\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/persistence.py\", line 866, in _emit_insert_statements\n",
      "    execute(statement, params)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/snorkel/udf.py\", line 160, in run\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/candidates.py\", line 93, in apply\n",
      "    for tc in self.matchers[i].apply(self.candidate_spaces[i].apply(self.session, context)):\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/snorkel/matchers.py\", line 75, in apply\n",
      "    for c in candidates:\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/tutorials/zeugma/zeugma_space.py\", line 185, in apply\n",
      "    for ts in OmniNgrams.apply(self, session, context):\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/candidates.py\", line 160, in apply\n",
      "    doc = session.query(Document).filter(Document.id == context.id).one()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/query.py\", line 2848, in one\n",
      "    ret = self.one_or_none()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/query.py\", line 2818, in one_or_none\n",
      "    ret = list(self)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/query.py\", line 2888, in __iter__\n",
      "    self.session._autoflush()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 1434, in _autoflush\n",
      "    self.flush()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2254, in flush\n",
      "    self._flush(objects)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2380, in _flush\n",
      "    transaction.rollback(_capture_exception=True)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/langhelpers.py\", line 66, in __exit__\n",
      "    compat.reraise(exc_type, exc_value, exc_tb)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/compat.py\", line 187, in reraise\n",
      "    raise value\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2344, in _flush\n",
      "    flush_context.execute()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/unitofwork.py\", line 391, in execute\n",
      "    rec.execute(self)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/unitofwork.py\", line 556, in execute\n",
      "    uow\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/persistence.py\", line 181, in save_obj\n",
      "    mapper, table, insert)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 948, in execute\n",
      "    return meth(self, multiparams, params)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py\", line 269, in _execute_on_connection\n",
      "    return connection._execute_clauseelement(self, multiparams, params)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1060, in _execute_clauseelement\n",
      "    compiled_sql, distilled_params\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1200, in _execute_context\n",
      "    context)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1416, in _handle_dbapi_exception\n",
      "    util.reraise(*exc_info)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/compat.py\", line 187, in reraise\n",
      "    raise value\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1193, in _execute_context\n",
      "    context)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/default.py\", line 507, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/usr/lib/python3.5/encodings/utf_8.py\", line 15, in decode\n",
      "    def decode(input, errors='strict'):\n",
      "KeyboardInterrupt\n",
      "Process CandidateExtractorUDF-30:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/snorkel/udf.py\", line 160, in run\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/candidates.py\", line 105, in apply\n",
      "    if not self.candidate_filter(tuple(args[i][1] for i in range(self.arity))):\n",
      "  File \"<ipython-input-52-68033f2d4b6b>\", line 6, in stg_temp_filter\n",
      "    if same_table((part, attr)):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/lf_helpers.py\", line 162, in same_table\n",
      "    for i in range(len(c))))\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/lf_helpers.py\", line 162, in <genexpr>\n",
      "    for i in range(len(c))))\n",
      "  File \"/home/weijiechen1994/Projects/Python/fonduer/fonduer/models/context.py\", line 202, in is_tabular\n",
      "    return self.table is not None\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/attributes.py\", line 242, in __get__\n",
      "    return self.impl.get(instance_state(instance), dict_)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/attributes.py\", line 599, in get\n",
      "    value = self.callable_(state, passive)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/strategies.py\", line 623, in _load_for_state\n",
      "    return self._emit_lazyload(session, state, ident_key, passive)\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/strategies.py\", line 710, in _emit_lazyload\n",
      "    session.query(self.mapper), ident_key)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/ext/baked.py\", line 495, in _load_on_ident\n",
      "    result = list(bq.for_session(self.session).params(**params))\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/ext/baked.py\", line 336, in __iter__\n",
      "    self.session._autoflush()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 1434, in _autoflush\n",
      "    self.flush()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2254, in flush\n",
      "    self._flush(objects)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2380, in _flush\n",
      "    transaction.rollback(_capture_exception=True)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/langhelpers.py\", line 66, in __exit__\n",
      "    compat.reraise(exc_type, exc_value, exc_tb)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/compat.py\", line 187, in reraise\n",
      "    raise value\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/session.py\", line 2344, in _flush\n",
      "    flush_context.execute()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/unitofwork.py\", line 391, in execute\n",
      "    rec.execute(self)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/unitofwork.py\", line 556, in execute\n",
      "    uow\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/persistence.py\", line 181, in save_obj\n",
      "    mapper, table, insert)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/orm/persistence.py\", line 866, in _emit_insert_statements\n",
      "    execute(statement, params)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 948, in execute\n",
      "    return meth(self, multiparams, params)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py\", line 269, in _execute_on_connection\n",
      "    return connection._execute_clauseelement(self, multiparams, params)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1060, in _execute_clauseelement\n",
      "    compiled_sql, distilled_params\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1200, in _execute_context\n",
      "    context)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1416, in _handle_dbapi_exception\n",
      "    util.reraise(*exc_info)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/util/compat.py\", line 187, in reraise\n",
      "    raise value\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/base.py\", line 1193, in _execute_context\n",
      "    context)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/sqlalchemy/engine/default.py\", line 507, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/usr/lib/python3.5/encodings/utf_8.py\", line 15, in decode\n",
      "    def decode(input, errors='strict'):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from fonduer import CandidateExtractor\n",
    "\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Cata_Labref, \n",
    "                        [cata_ngrams, labref_ngrams], \n",
    "                        [cata_matcher, labref_matcher], \n",
    "                        candidate_filter=candidate_filter)\n",
    "\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specified that these `Candidates` belong to the training set by specifying `split=0`; recall that we're referring to train/dev/test as splits 0/1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 0\n"
     ]
    }
   ],
   "source": [
    "train_cands = session.query(Part_Attr).filter(Part_Attr.split == 0).all()\n",
    "print(\"Number of candidates:\", len(train_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
